# Test-driven Jailbreak Induction in Code Execution Agents

This repository provides the official implementation of the paper  
**â€œTest-driven Jailbreak Induction in Code Execution Agentsâ€**.

The project studies how test-case generation workflows can be exploited to induce unintended or unsafe behaviors in LLM-based code execution agents.

---

## ğŸ” Overview

Recent LLM-based code agents can autonomously write and execute programs inside sandboxed environments.  
However, their **test-case generation and debugging workflows** may unintentionally create opportunities for jailbreak-like behaviors.

This project proposes a **test-driven jailbreak induction framework**, which:

- Constructs realistic execution scenarios
- Prompts agents to generate and run test harnesses
- Observes whether unsafe or unintended behaviors occur
- Applies rollback strategies to iteratively induce execution

The framework enables systematic evaluation of **agent robustness and safety mechanisms**.

---

## ğŸ§© Pipeline Structure

The evaluation pipeline contains two main modes:

### 1. Direct Code Execution (RA)
The agent receives a code snippet and decides whether to:

- Execute it
- Modify it
- Reject it
- Finish the task

This evaluates baseline safety behavior.

### 2. Test-Case-Driven Execution (RA_TEST)
The agent is prompted to:

1. Generate a test harness
2. Execute the harness
3. Observe outcomes
4. Apply rollback strategies if rejected or failed

Four test-case generation modes are used:

- **Validation test**
- **Crash reproduction test**
- **Unit test**
- **Regression test**

If the agent rejects or fails:

- A **semantic rollback** modifies narrative framing
- An **execution rollback** strengthens test procedures

This iterative process increases the likelihood of inducing unsafe execution.

---

## ğŸ“ Directory Structure

```bash
.
â”œâ”€â”€ README.md
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ RedCode-Exec/              # Python and Bash task datasets
â”‚       â”œâ”€â”€ py2text_dataset_json/
â”‚       â””â”€â”€ bash2text_dataset_json/
â”‚
â”œâ”€â”€ environment/                   # Docker sandbox environment
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ init.sh
â”‚   â”œâ”€â”€ start_processes.sh
â”‚   â””â”€â”€ sensitive system files
â”‚
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ RedCode_Exec/
â”‚       â”œâ”€â”€ main.py                # Entry point for evaluation
â”‚       â”œâ”€â”€ evaluation.py          # Scoring logic
â”‚       â”œâ”€â”€ RA_evaluation/         # Direct execution pipeline
â”‚       â””â”€â”€ RA_evaluation_test/    # Test-driven jailbreak pipeline
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ RA_eval.sh
â”‚   â”œâ”€â”€ RA_TEST_eval.sh
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ environment.yml                # Python environment configuration
